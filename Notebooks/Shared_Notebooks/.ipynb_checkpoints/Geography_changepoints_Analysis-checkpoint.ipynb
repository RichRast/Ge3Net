{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hyperparameters</b>: \n",
    "<br><b>win_tol</b>: window tolerance within which if a method detected cp, count that as true changepoint detected\n",
    "<br>win_tol=2\n",
    "<br>For BOCD: <b>run_length_threshold</b> - the change in run_length that constitutes a changepoint, for example if run_length_threshold is set to 4, then if run length drops by 4 or more, then that constitutes a changepoint\n",
    "<br>For gradient based: <b>gradient_threshold</b>: the threshold above which a change in gradient constitutes a changepoint.\n",
    "<br>For MC_dropout: <b>mc_dropout_threshold</b>: the threshold above which a change in variance in any of the output dimensions constitutes a changepoint\n",
    "<br>\n",
    "<br> <b>If TP=FP=0, Precision=1 rather than 0 and if TP=FN=0, then Recall=1 rather than 0 \n",
    "<br> zero division is taken as 1 and not 0 for Precision and Recall</b>\n",
    "<br> example: true_cps = [3, 31, 250, 273], pred_cps = [30, 31, 249, 250, 272, 300]\n",
    "<br> TP = 3, FP = 1, FN = 1, TN = 317-5 = 312\n",
    "<br> true changepoint 3 could not be detected but the rest were detected within the window tolerance\n",
    "<br> false changepoint 300 was detected\n",
    "<br>\n",
    "<br>Precision = TP/(TP+FP)\n",
    "<br>Recall or A_cp = TP/(TP+FN)\n",
    "<br>A_cp = Accuracy for detecting changepoints (minority class)\n",
    "<br>Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "<br>A_major or A_no_cp = TN/(TN+FP)\n",
    "<br>A_major or A_no_cp = Accuracy for detecting points that are not changepoints (majority class)\n",
    "<br>Balanced_Accuracy = 1/2*(A_cp+A_major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import ticker\n",
    "import matplotlib as mpl\n",
    "from   matplotlib.colors import LogNorm\n",
    "from sklearn.metrics import precision_recall_curve, recall_score, precision_score, \\\n",
    "balanced_accuracy_score, accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"/home/users/richras/Ge2Net_Repo\"\n",
    "os.chdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set environment variables\r\n",
      "All done\r\n"
     ]
    }
   ],
   "source": [
    "!./ini.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['USER_PATH']='/home/users/richras/Ge2Net_Repo'\n",
    "os.environ['USER_SCRATCH_PATH']=\"/scratch/users/richras\"\n",
    "os.environ['IN_PATH']='/scratch/groups/cdbustam/richras/data_in'\n",
    "os.environ['OUT_PATH']='/scratch/groups/cdbustam/richras/data_out'\n",
    "os.environ['LOG_PATH']='/scratch/groups/cdbustam/richras/logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.utils.dataUtil import load_path, save_file, vcf2npy, get_recomb_rate, interpolate_genetic_pos, form_windows\n",
    "from src.utils.modelUtil import Params, load_model\n",
    "from src.utils.decorators import timer\n",
    "from src.models import AuxiliaryTask, LSTM, Attention, BasicBlock, Model_A, Model_B, Model_C, BOCD\n",
    "from src.models.distributions import Multivariate_Gaussian\n",
    "from src.main.evaluation import eval_cp_batch, reportChangePointMetrics, t_prMetrics, cpMethod, eval_cp_matrix, \\\n",
    "getCpPred\n",
    "import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model and use valid data to choose hyperpparams for the different cp methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify the dataset to be evaluated\n",
    "# chm22 pca full dataset and model\n",
    "labels_path = osp.join(os.environ['OUT_PATH'],'humans/labels/data_id_1_geo')\n",
    "data_path = osp.join(os.environ['OUT_PATH'],'humans/labels/data_id_1_geo')\n",
    "models_path=osp.join(os.environ['OUT_PATH'],'humans/training/Model_B_exp_id_32_data_id_1_geo/') \n",
    "dataset_type='valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " device used: cuda\n",
      "Loading the datasets...\n"
     ]
    }
   ],
   "source": [
    "config={}\n",
    "config['data.labels']=labels_path \n",
    "config['data.dir']=data_path \n",
    "config['models.dir']=models_path\n",
    "config['data.dataset_type']=dataset_type\n",
    "config['cuda']='cuda'\n",
    "config['model.loadBest']=True\n",
    "json_path = osp.join(config['models.dir'], 'params.json')\n",
    "assert osp.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = Params(json_path)\n",
    "params.rtnOuts=True\n",
    "params.mc_dropout=True\n",
    "params.mc_samples=100\n",
    "params.cp_tol=0\n",
    "params.evalCp=True\n",
    "params.evalBalancedGcd=True\n",
    "params.evalExtraMainLosses=True\n",
    "results, valid_dataset, _, modelStats=test.main(config, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.t_out.coord_main=results.t_out.coord_main.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.t_out.coord_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.t_accr, results.t_cp_accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def prMetricsByThresh(method_name, cp_pred_raw, cp_target, steps, minThresh, maxThresh, win_tol=2, byWindows=False):\n",
    "    increment = (maxThresh - minThresh)/steps\n",
    "    df=pd.DataFrame(columns=list(t_prMetrics._fields)+['thresh'])\n",
    "    for thresh in np.arange(minThresh, maxThresh + increment, increment):\n",
    "        prMetrics, cp_pred = reportChangePointMetrics(method_name, cp_pred_raw, cp_target, thresh, win_tol)\n",
    "        if byWindows: \n",
    "            prMetrics={}\n",
    "            prMetrics['Precision']=precision_score(cp_target.flatten(), cp_pred.flatten())\n",
    "            prMetrics['Recall']=recall_score(cp_target.flatten(), cp_pred.flatten())\n",
    "            prMetrics['BalancedAccuracy']=balanced_accuracy_score(cp_target.flatten(), cp_pred.flatten())\n",
    "            prMetrics['Accuracy']=accuracy_score(cp_target.flatten(), cp_pred.flatten())\n",
    "            prMetrics['A_major']=(2*prMetrics['BalancedAccuracy'])-prMetrics['Recall']\n",
    "        prMetrics['thresh']=thresh\n",
    "        prMetrics['F1']=2*prMetrics['Precision']*prMetrics['Recall']/(prMetrics['Precision']+prMetrics['Recall'])\n",
    "        df=df.append(prMetrics, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network, choose the threshold for logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn=prMetricsByThresh(cpMethod.neural_network.name, torch.tensor(results.t_out.cp_logits).float(), \\\n",
    "                        valid_dataset.data['cps'].unsqueeze(2).float(), 20, 0.4, 0.5)\n",
    "df_nn.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnThresh=0.455"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient method, choose the threshold for gradient difference in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad=prMetricsByThresh(cpMethod.gradient.name, torch.tensor(results.t_out.coord_main).float(), \\\n",
    "                        valid_dataset.data['cps'].unsqueeze(2).float(), 20, 0,1)\n",
    "df_grad.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradThresh=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mc dropout variance, choose the threshold for difference in variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var=prMetricsByThresh(cpMethod.mc_dropout.name, torch.tensor(results.t_out.y_var).float(), \\\n",
    "                        valid_dataset.data['cps'].unsqueeze(2).float(), 20, 0,0.5)\n",
    "df_var.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "varThresh=0.025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOCD, Compute the run lengths and changepoints by using a multivariate Gaussian (with independent three dimensions of predictions) to model the prediction sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = results.t_out.coord_main\n",
    "n_vec_dim=y_pred.shape[-1]\n",
    "data_tensor = torch.tensor(y_pred).float()\n",
    "batch_size_cpd = data_tensor.shape[0]\n",
    "mu_prior = torch.zeros((batch_size_cpd, 1,n_vec_dim))\n",
    "mean_var=torch.mean(torch.var(data_tensor, dim =1),dim=0).unsqueeze(0)\n",
    "cov_prior = (mean_var.repeat(batch_size_cpd,1).unsqueeze(1)* torch.eye(n_vec_dim)).reshape(batch_size_cpd,1,n_vec_dim,n_vec_dim)\n",
    "cov_x = cov_prior\n",
    "likelihood_model = Multivariate_Gaussian(mu_prior, cov_prior, cov_x)\n",
    "T = params.n_win\n",
    "model_cpd = BOCD.BOCD(None, T, likelihood_model, batch_size_cpd)\n",
    "posterior, _, predictive, e_mean = model_cpd.run_recursive(data_tensor, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bocd=prMetricsByThresh(cpMethod.BOCD.name, model_cpd.cp, \\\n",
    "                        valid_dataset.data['cps'].unsqueeze(2).float(), 20,0,10)\n",
    "df_bocd.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bocdThresh=2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Table with chosen thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format='{:,.4f}'.format\n",
    "df_summary=pd.DataFrame(columns=list(t_prMetrics._fields)+['thresh'])\n",
    "cpThresh=[nnThresh, gradThresh, varThresh, bocdThresh]\n",
    "cp_target=valid_dataset.data['cps']\n",
    "cp_pred_raw=[torch.tensor(results.t_out.cp_logits).float(), torch.tensor(results.t_out.coord_main).float(), \n",
    "             torch.tensor(results.t_out.y_var).float(), model_cpd.cp]\n",
    "for name, thresh, pred_raw in zip([cp.name for cp in cpMethod], cpThresh, cp_pred_raw):\n",
    "    prMetrics, _= reportChangePointMetrics(name, pred_raw, cp_target, thresh, win_tol=2)\n",
    "    prMetrics['thresh']=thresh\n",
    "    prMetrics['F1']=2*prMetrics['Precision']*prMetrics['Recall']/(prMetrics['Precision']+prMetrics['Recall'])\n",
    "    prMetrics['Name']=name\n",
    "    df_summary=df_summary.append(prMetrics, ignore_index=True)\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot of Precision/Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=[12,12])\n",
    "\n",
    "for (row,col), df, name in zip(itertools.product([0,1],[0,1]),[df_nn, df_grad, df_var, df_bocd],[cp.name for cp in cpMethod]) :\n",
    "    ax[row,col].plot(df['Precision'], df['Recall'])\n",
    "    ax[row,col].set_xlabel('Precision')\n",
    "    ax[row,col].set_ylabel('Recall')\n",
    "    ax[row,col].set_title(name)\n",
    "    df_row=df_summary[df_summary.Name==name]\n",
    "    ax[row,col].scatter(df_row['Precision'], df_row['Recall'], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[10,8])\n",
    "\n",
    "for df, name in zip([df_nn, df_grad, df_var, df_bocd],[cp.name for cp in cpMethod]) :\n",
    "    aucScore = \"N/A\"\n",
    "    if name!=\"BOCD\":\n",
    "        aucScore = format(auc(df['Recall'], df['Precision'])*100, \".2f\")\n",
    "    plt.plot(df['Recall'], df['Precision'], label=str(name) + \" AUC=\"+ str(aucScore)+\"%\")\n",
    "    plt.ylabel('Precision', fontsize=15)\n",
    "    plt.xlabel('Recall', fontsize=15)\n",
    "    plt.title(\"Comparison of different methods for changepoint detection\")\n",
    "    df_row=df_summary[df_summary.Name==name]\n",
    "    plt.scatter(df_row['Recall'], df_row['Precision'], color='r')\n",
    "    plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Qualitative by looking at anecdotes/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "granular_pop_dict = load_path(osp.join(labels_path, 'granular_pop.pkl'), en_pickle=True)\n",
    "cp_target=valid_dataset.data['cps']\n",
    "seqlen=cp_target.shape[1]\n",
    "rev_pop_dict={v:k for k,v in granular_pop_dict.items()}\n",
    "cp_pred_raw=[torch.tensor(results.t_out.cp_logits).float(), torch.tensor(results.t_out.coord_main).float(), \n",
    "             torch.tensor(results.t_out.y_var).float(), model_cpd.cp]\n",
    "pred_cps = {}\n",
    "for name, thresh, pred_raw in zip([cp.name for cp in cpMethod], cpThresh, cp_pred_raw):\n",
    "    _, pred_cps[name]= reportChangePointMetrics(name, pred_raw, cp_target, thresh)\n",
    "\n",
    "true_cps=cp_target.detach().cpu().numpy()\n",
    "pred_cps={k:v.detach().cpu().numpy() for k,v in pred_cps.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_changepoints(true_cps, pred_cps, y_pred, bocp_rl, y_var, popNames):\n",
    "    fig, ax = plt.subplots(8,1,figsize=[18,30])\n",
    "    ax[0].plot(y_pred)\n",
    "    ax[0].text(0, np.max(y_pred)-0.5, s=popNames[0], size=10)\n",
    "    for i in np.nonzero(true_cps)[0]:\n",
    "        ax[0].plot([i,i], [np.min(y_pred), np.max(y_pred)], 'r' )\n",
    "        ax[0].text(i, np.max(y_pred)-0.5, s=popNames[i+1], size=10)\n",
    "    ax[1].plot(pred_cps[cpMethod.gradient.name])\n",
    "    ax[2].plot(y_var)\n",
    "    ax[3].plot(pred_cps[cpMethod.mc_dropout.name])\n",
    "    ax[4].plot(pred_cps[cpMethod.neural_network.name])\n",
    "    ax[5].plot(np.arange(T+1), bocp_rl)\n",
    "    ax[6].plot(pred_cps[cpMethod.BOCD.name])\n",
    "    ax[7].plot(true_cps)\n",
    "    \n",
    "    ax[0].set_title(\"n_vectors\")\n",
    "    ax[1].set_title(\"Simple gradient(post_process)\")\n",
    "    ax[2].set_title(\"Mc dropout variance\")\n",
    "    ax[3].set_title(\"Mc dropout\")\n",
    "    ax[4].set_title(\"Neural Network predicted cp\")\n",
    "    ax[5].set_title(\"BOCD (post process) run_length\")\n",
    "    ax[6].set_title(\"BOCD (post process) cp\")\n",
    "    ax[7].set_title(\"True Cps\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = 2000\n",
    "true_cpsSample=true_cps[index,:]\n",
    "pred_cpsSample={k:v[index,:] for k,v in pred_cps.items()}\n",
    "y_predSample=results.t_out.coord_main[index,:]\n",
    "y_varSample=results.t_out.y_var[index,:,:]\n",
    "bocp_rlSample=model_cpd.cp.detach().cpu().numpy()[index,:]\n",
    "granularpopSample=valid_dataset.data['granular_pop'][index,:].detach().cpu().numpy()\n",
    "namesSample=[rev_pop_dict[i] for i in granularpopSample.astype(int)]\n",
    "plot_changepoints(true_cpsSample, pred_cpsSample, y_predSample, bocp_rlSample, y_varSample, namesSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2620\n",
    "true_cpsSample=true_cps[index,:]\n",
    "pred_cpsSample={k:v[index,:] for k,v in pred_cps.items()}\n",
    "y_predSample=results.t_out.coord_main[index,:]\n",
    "y_varSample=results.t_out.y_var[index,:,:]\n",
    "bocp_rlSample=model_cpd.cp.detach().cpu().numpy()[index,:]\n",
    "granularpopSample=valid_dataset.data['granular_pop'][index,:].detach().cpu().numpy()\n",
    "namesSample=[rev_pop_dict[i] for i in granularpopSample.astype(int)]\n",
    "plot_changepoints(true_cpsSample, pred_cpsSample, y_predSample, bocp_rlSample, y_varSample, namesSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1200\n",
    "true_cpsSample=true_cps[index,:]\n",
    "pred_cpsSample={k:v[index,:] for k,v in pred_cps.items()}\n",
    "y_predSample=results.t_out.coord_main[index,:]\n",
    "y_varSample=results.t_out.y_var[index,:,:]\n",
    "bocp_rlSample=model_cpd.cp.detach().cpu().numpy()[index,:]\n",
    "granularpopSample=valid_dataset.data['granular_pop'][index,:].detach().cpu().numpy()\n",
    "namesSample=[rev_pop_dict[i] for i in granularpopSample.astype(int)]\n",
    "plot_changepoints(true_cpsSample, pred_cpsSample, y_predSample, bocp_rlSample, y_varSample, namesSample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate on a subset of samples that have at least 1 changepoint \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_idx=torch.unique(torch.nonzero(valid_dataset.data['cps'])[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_subset=pd.DataFrame(columns=list(t_prMetrics._fields)+['thresh'])\n",
    "subsetSamples_trueCps=valid_dataset.data['cps'].unsqueeze(2)[subset_idx,:]\n",
    "cp_pred_raw=[torch.tensor(results.t_out.cp_logits[subset_idx,:]).float(), \\\n",
    "             torch.tensor(results.t_out.coord_main[subset_idx,:]).float(), \n",
    "             torch.tensor(results.t_out.y_var[subset_idx,:]).float(),\\\n",
    "             model_cpd.cp[subset_idx,:]]\n",
    "for name, thresh, pred_raw in zip([cp.name for cp in cpMethod], cpThresh, cp_pred_raw):\n",
    "    prMetrics, _= reportChangePointMetrics(name, pred_raw, subsetSamples_trueCps, thresh, win_tol=2)\n",
    "    prMetrics['thresh']=thresh\n",
    "    prMetrics['F1']=2*prMetrics['Precision']*prMetrics['Recall']/(prMetrics['Precision']+prMetrics['Recall'])\n",
    "    prMetrics['Name']=name\n",
    "    df_summary_subset=df_summary_subset.append(prMetrics, ignore_index=True)\n",
    "df_summary_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute metrics by placing win_tol=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zero_tol=pd.DataFrame(columns=list(t_prMetrics._fields)+['thresh'])\n",
    "trueCps=valid_dataset.data['cps'].unsqueeze(2).float()\n",
    "cp_pred_raw=[torch.tensor(results.t_out.cp_logits).float(), \\\n",
    "             torch.tensor(results.t_out.coord_main).float(), \n",
    "             torch.tensor(results.t_out.y_var).float(),\\\n",
    "             model_cpd.cp]\n",
    "for name, thresh, pred_raw in zip([cp.name for cp in cpMethod], cpThresh, cp_pred_raw):\n",
    "    prMetrics, _= reportChangePointMetrics(name, pred_raw, trueCps, thresh, win_tol=0)\n",
    "    prMetrics['thresh']=thresh\n",
    "    prMetrics['F1']=2*prMetrics['Precision']*prMetrics['Recall']/(prMetrics['Precision']+prMetrics['Recall'])\n",
    "    prMetrics['Name']=name\n",
    "    df_zero_tol=df_zero_tol.append(prMetrics, ignore_index=True)\n",
    "df_zero_tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate to make sure matches with scikit learn (subset with samples that have cp, and win_tol=0 for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_subset=pd.DataFrame(columns=list(t_prMetrics._fields)+['thresh'])\n",
    "subsetSamples_trueCps=valid_dataset.data['cps'].unsqueeze(2)[subset_idx,:]\n",
    "cp_pred_raw=[torch.tensor(results.t_out.cp_logits[subset_idx,:]).float(), \\\n",
    "             torch.tensor(results.t_out.coord_main[subset_idx,:]).float(), \n",
    "             torch.tensor(results.t_out.y_var[subset_idx,:]).float(),\\\n",
    "             model_cpd.cp[subset_idx,:]]\n",
    "for name, thresh, pred_raw in zip([cp.name for cp in cpMethod], cpThresh, cp_pred_raw):\n",
    "    prMetrics, _= reportChangePointMetrics(name, pred_raw, subsetSamples_trueCps, thresh, win_tol=0)\n",
    "    prMetrics['thresh']=thresh\n",
    "    prMetrics['F1']=2*prMetrics['Precision']*prMetrics['Recall']/(prMetrics['Precision']+prMetrics['Recall'])\n",
    "    prMetrics['Name']=name\n",
    "    df_summary_subset=df_summary_subset.append(prMetrics, ignore_index=True)\n",
    "df_summary_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate by Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn=prMetricsByThresh(cpMethod.neural_network.name, torch.tensor(results.t_out.cp_logits).float(), \\\n",
    "                        valid_dataset.data['cps'].unsqueeze(2).float(), 20, 0.4, 0.5, byWindows=True)\n",
    "df_nn.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad=prMetricsByThresh(cpMethod.gradient.name, torch.tensor(results.t_out.coord_main).float(), \\\n",
    "                        valid_dataset.data['cps'].unsqueeze(2).float(), 20, 0,1, byWindows=True)\n",
    "df_grad.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var=prMetricsByThresh(cpMethod.mc_dropout.name, torch.tensor(results.t_out.y_var).float(), \\\n",
    "                        valid_dataset.data['cps'].unsqueeze(2).float(), 20, 0,0.5, byWindows=True)\n",
    "df_var.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bocd=prMetricsByThresh(cpMethod.BOCD.name, model_cpd.cp, \\\n",
    "                        valid_dataset.data['cps'].unsqueeze(2).float(), 20,1,11, byWindows=True)\n",
    "df_bocd.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=[12,12])\n",
    "\n",
    "for (row,col), df, name in zip(itertools.product([0,1],[0,1]),[df_nn, df_grad, df_var, df_bocd],[cp.name for cp in cpMethod]) :\n",
    "    ax[row,col].plot(df['Precision'], df['Recall'])\n",
    "    ax[row,col].set_xlabel('Precision')\n",
    "    ax[row,col].set_ylabel('Recall')\n",
    "    ax[row,col].set_title(name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model and evaluate with test data on the chosen hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify the dataset to be evaluated\n",
    "# chm22 pca full dataset and model\n",
    "labels_path = osp.join(os.environ['OUT_PATH'],'humans/labels/data_id_1_geo')\n",
    "data_path = osp.join(os.environ['OUT_PATH'],'humans/labels/data_id_1_geo')\n",
    "models_path=osp.join(os.environ['OUT_PATH'],'humans/training/Model_B_exp_id_32_data_id_1_geo/') \n",
    "dataset_type='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={}\n",
    "config['data.labels']=labels_path \n",
    "config['data.dir']=data_path \n",
    "config['models.dir']=models_path\n",
    "config['data.dataset_type']=dataset_type\n",
    "config['cuda']='cuda'\n",
    "config['model.loadBest']=False\n",
    "json_path = osp.join(config['models.dir'], 'params.json')\n",
    "assert osp.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = Params(json_path)\n",
    "params.rtnOuts=True\n",
    "params.mc_dropout=True\n",
    "params.mc_samples=100\n",
    "params.cp_tol=0\n",
    "results, test_dataset=test.main(config, params)\n",
    "results.t_out.coord_main=results.t_out.coord_main.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Table with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute BOCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = results.t_out.coord_main\n",
    "n_vec_dim=y_pred.shape[-1]\n",
    "test_tensor = torch.tensor(y_pred).float()\n",
    "batch_size_cpd = test_tensor.shape[0]\n",
    "mu_prior = torch.zeros((batch_size_cpd, 1,n_vec_dim))\n",
    "cov_prior = (mean_var.repeat(batch_size_cpd,1).unsqueeze(1)* torch.eye(n_vec_dim)).reshape(batch_size_cpd,1,n_vec_dim,n_vec_dim)\n",
    "cov_x = cov_prior\n",
    "likelihood_model = Multivariate_Gaussian(mu_prior, cov_prior, cov_x)\n",
    "T = params.n_win\n",
    "model_cpd = BOCD.BOCD(None, T, likelihood_model, batch_size_cpd)\n",
    "posterior, _, predictive, e_mean = model_cpd.run_recursive(test_tensor, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.DataFrame(columns=list(t_prMetrics._fields)+['thresh'])\n",
    "trueCps=test_dataset.data['cps'].unsqueeze(2).float()\n",
    "cp_pred_raw=[torch.tensor(results.t_out.cp_logits).float(), \\\n",
    "             torch.tensor(results.t_out.coord_main).float(), \n",
    "             torch.tensor(results.t_out.y_var).float(),\\\n",
    "             model_cpd.cp]\n",
    "for name, thresh, pred_raw in zip([cp.name for cp in cpMethod], cpThresh, cp_pred_raw):\n",
    "    prMetrics, _= reportChangePointMetrics(name, pred_raw, trueCps, thresh, win_tol=2)\n",
    "    prMetrics['thresh']=thresh\n",
    "    prMetrics['F1']=2*prMetrics['Precision']*prMetrics['Recall']/(prMetrics['Precision']+prMetrics['Recall'])\n",
    "    prMetrics['Name']=name\n",
    "    df_test=df_test.append(prMetrics, ignore_index=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
